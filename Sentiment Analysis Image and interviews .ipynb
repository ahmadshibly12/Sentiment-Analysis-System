{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # Computer vision Libary\n",
    "from deepface import DeepFace  # DeepFace is a deep learning facial recognition system created by a research group at Facebook. \n",
    "# It identifies human faces in digital images. It employs a nine-layer neural network with over 120 million connection \n",
    "# weights and was trained on four million images uploaded by Facebook users. The Facebook Research team has stated that \n",
    "# the DeepFace method reaches an accuracy of 97.35% ± 0.25% on Labeled Faces in the Wild (LFW) data set where human \n",
    "# beings have 97.53%. This means that DeepFace is sometimes more successful than the human beings.\n",
    "\n",
    "\n",
    "import os\n",
    "# tool used to deal with filenames, paths, directories. These modules are wrappers for platform-specific modules, it work on UNIX, Windows, \n",
    "# Mac OS, and any other platform supported by Python\n",
    "\n",
    "# the t and k in tkinter should be lower case letters to work in python v3\n",
    "from tkinter import Tk    # from tkinter import Tk for Python 3.x\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "\n",
    "# tkinter is used to GUI, and to facilitate to the user the ability to choose files and destinations he/she wants to use inside the code for several tasks\n",
    "import matplotlib.pyplot as plt # used for plotting our images and data\n",
    "import time # used to time the output in a certain time slots and periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Sentiment Analysis on social media pictures or posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root = Tk() # initiate the window to the GUI\n",
    "\n",
    "Imageopen= askopenfilename(parent=root, title='Choose an image for sentiment detection')\n",
    "# show an \"Open\" dialog box and return the path to the selected file\n",
    "# parent=root makes the window appear in the front screen and in top of all windows, rather than appearing outside in the desktop\n",
    "\n",
    "root.withdraw() # close the dialog after being done with it, to reduce the number of dialogs used and for a neater work and more organized implementation because of its usage later on in the code\n",
    "\n",
    "# reading the figure chosen by the user from his pc where he used the dialog\n",
    "image=cv2.imread(Imageopen)\n",
    "\n",
    "plt.imshow(image) # BGR Pattern visualization of the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) \n",
    "# Color Fix from BGR to RGB for clearer viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_face = DeepFace.detectFace(image, detector_backend = 'mtcnn')\n",
    "# using the famous up-to-date library Deepface for detecting the facethis action is built in inside the library, it takes \n",
    "#the parameter which is the image, and the detector backend type which is mtcnn and can be other depending on the user desire and application\n",
    "plt.imshow(detected_face) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our case we only care about sentiments and dominant sentiments or emotions so we can specify the parameter of actions \n",
    "# into only emotion, thus analyzing the image in terms of emotions only and nothing else\n",
    "predictemotion1=DeepFace.analyze(detected_face,actions=['emotion'],enforce_detection=False)\n",
    "predictemotion1\n",
    "# So we specifiy the actions to only emotions category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You notice that the results are not logically true, thats why we should multiply the detected face image by 255 to get \n",
    "# a suitable sie that this built in function needs for analyzing and detection\n",
    "predictemotion1=DeepFace.analyze(detected_face*255,actions=['emotion'],enforce_detection=False)\n",
    "predictemotion1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the major built in action of the library which is .analyze , this action will detect several criteria about this image\n",
    "# it will detect age, race,gender and emotions and dominant emotion with their corresponding values\n",
    "predictemotion2=DeepFace.analyze(detected_face*255,enforce_detection=False)\n",
    "predictemotion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Sentiment analysis of videos and interviews for example\n",
    "\n",
    "# we will use this library because of its tools, and one of them is \n",
    "# the ffmpeg_extract_subclip which allows the user to crop the videos\n",
    "# and decompose them into parts, according to time intervals chosen by\n",
    "# the user according to the application or the desire behind this program\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "\n",
    "root=Tk()\n",
    "Interviewvideo= askopenfilename(parent=root, title='Select the video you want to crop')\n",
    "root.withdraw()\n",
    "Interview=cv2.imread(Interviewvideo)\n",
    "x= int(input(\"To how many parts do you want your video to be splitted?\"))\n",
    "i=1\n",
    "# While loop to decompose the video into a number of parts a user chooses\n",
    "while i < x+1:\n",
    "    # The parameters are the original video, starting time of the cropped video\n",
    "    # end time of the video, and then the destination of the cropped video\n",
    "    ffmpeg_extract_subclip(Interviewvideo, int(input(\"Start Time of video\" + str(i)+\": \")), int(input(\"End Time of video\"+ \n",
    "                                                                    str(i)+\": \")),input('File Destination and name: '))\n",
    "    i+=1\n",
    "# In my case  I made a sentiment analysis of Joe Biden’s 60-minute 2020 Election interview I chose the two parts where \n",
    "# Joe biden was asked “Do you think trump would win?” and the second question was “Do you think raising taxes is a \n",
    "# good idea in this crisis? “\n",
    "\n",
    "# Then i choose my time intervals accordingly ( the inputs should be in seconds so if the start time of the video is minute 5 the input should be 5*60=300)\n",
    "\n",
    "\n",
    "\n",
    "# this can be used during investigations, interviews, and during talk shows with celebrities and politicians so you can try to see\n",
    "# which video is affective and which one is normal and varies according to the target behind the application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I imported VideoFileCLip which i will use later on here in this block of\n",
    "# code, to detect the duration of the video in seconds\n",
    "from moviepy.editor import VideoFileClip\n",
    "import pandas as pd\n",
    "array=[]\n",
    "\n",
    "# User inputs the number of videos he want to split into frames\n",
    "videosnum=input('How many videos do you want to split into frames?')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Path of the frames to be saved inside\n",
    "y=input('Path of the frames: ')\n",
    "# Path of the detected faces of each frame which will be used later\n",
    "# for sentiment detection\n",
    "v=input('Path of the Detected faces: ')\n",
    "r=0\n",
    "# This while loop ends when the user splits all the targeted number of videos\n",
    "while(r<int(videosnum)):\n",
    "    # Each time we start with a new video, a new array will be appended for this specific video\n",
    "    array.append([])\n",
    "    # Selecting the video the user wants to split\n",
    "    root=Tk()\n",
    "    x= askopenfilename(parent=root, title='Select the video '+str(r+1)+' you want to split into frames')\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Capturing the video and using the function videofileclip to detect\n",
    "    # the duration of the video, which will be used in the bottom\n",
    "    cap = cv2.VideoCapture(x)\n",
    "    clip = VideoFileClip(x)\n",
    "    clip.duration\n",
    "    # Detection whether the path exists inside the pc or not\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory of data')\n",
    "    \n",
    "    # In this application im using the idea of decomposing my video to each\n",
    "    # second so 1 frame per second, so i will take the variable framenumber=1\n",
    "    framenumber=1\n",
    "    # current frame state\n",
    "    currentFrame = 0\n",
    "    # this is the fps of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # the user inputs the names of the frames associated to each video\n",
    "    x=input('Frames name: ')\n",
    "    # while loop that will end once the framenumbers are more than the duration\n",
    "    # for example if the video is 15seconds, and we already got 15frames then \n",
    "    # the loop will end because we already satisfied the target which is 1 frame\n",
    "    # per second of the video\n",
    "    while framenumber<=float(clip.duration):\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES,currentFrame)\n",
    "        ret, frame = cap.read()\n",
    "        # Saves image of the current frame in jpg file\n",
    "        name =y+str(x)+ 'Frame'+str(framenumber) + '.jpg'\n",
    "        print ('Creating...' + name)\n",
    "        # saving the frames into the path y that is previously selected by the user\n",
    "        # and using the name with the frame number indicated above and the \n",
    "        # type of the file or image\n",
    "        cv2.imwrite(name, frame)\n",
    "        # reading the saved picture\n",
    "        pic=cv2.imread(name)\n",
    "        # detecting the face only\n",
    "        detected_faces = DeepFace.detectFace(pic,detector_backend = 'mtcnn')\n",
    "        # changing the color from BGR to RGB for better visualization\n",
    "        d=cv2.cvtColor(detected_faces*255, cv2.COLOR_BGR2RGB)\n",
    "        # saving the detected face of this speicific frame into the path\n",
    "        # selected previously by the user, 255*d is used to avoid black screen issue\n",
    "        name2=v+'DetectedFace of '+str(x)+ 'Frame '+str(framenumber) + '.jpg'\n",
    "        e=cv2.imwrite(name2,d)\n",
    "        # analyzing the detected face and indicating the dominant emotion\n",
    "        predict=DeepFace.analyze(name2,actions=['emotion'],enforce_detection=False)\n",
    "        # Append the dominant sentiment indicated in the array\n",
    "        array[r].append(predict['dominant_emotion'])\n",
    "        \n",
    "        \n",
    "        # To stop duplicate images\n",
    "        # increment frame number to the next frame\n",
    "        framenumber+=1\n",
    "        # currentframe is equal to the current one + the fps of the video\n",
    "        # that way we are moving every one second\n",
    "        currentFrame += fps\n",
    "        # output each 1 second\n",
    "        time.sleep(1)\n",
    "    r+=1\n",
    "    # incrementing r means that we moved to the next cropped video\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go through each array that has the sentiments of each frame and we print out the arrays with their content\n",
    "k=0\n",
    "while k<int(videosnum):\n",
    "    print('Video Number '+str(k)+\" :\")\n",
    "    print(array[k])\n",
    "    print('\\n')\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will try to average the occurring of each sentiment inside a video\n",
    "j=0\n",
    "dominance=[]\n",
    "# All the sentiments\n",
    "Sentiments=['angry','sad','happy','fear','surprise','neutral','disgust']\n",
    "# looping through all videos\n",
    "while j<int(videosnum):\n",
    "    dominance.append([])\n",
    "    print('Video Number '+str(j)+':')\n",
    "    print('\\n')\n",
    "    for i in Sentiments:\n",
    "        # Average which is equal to the count of each unique element inside the array divided by the length of the array\n",
    "        av1=(array[j].count(i))/len(array[j])\n",
    "        # Average converted to percentage\n",
    "        av=av1*100\n",
    "        # If the average is not 0% we add it to the dominance array\n",
    "        if av1!=0:\n",
    "            g=i+' ' +str(av)+' % '\n",
    "            dominance[j].append(g)\n",
    "        # if the average is 0% then the sentiment is not compatible and has no use in our application\n",
    "        else:\n",
    "            print(str(i)+' was removed because its doesnt represnt a sentiment of the frame')\n",
    "    print('\\n')\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we loop through all the arrays and print the dominant sentiments with their percentage\n",
    "h=0\n",
    "while h<int(videosnum):\n",
    "    print('Video Number '+str(h)+' :')\n",
    "    print('\\n')\n",
    "    print('Number of frames: '+str(len(array[h])))\n",
    "    print('Duration: '+str(len(array[h]))+'seconds')\n",
    "    print('Dominant Sentiments: ')\n",
    "    print(dominance[h])\n",
    "    print('\\n')\n",
    "    h+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

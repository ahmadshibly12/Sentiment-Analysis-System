{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries that we are going to use in the project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# We will be using two datasets, the first one is used on all classifiers, while the second one will be used for Naive Bayes \n",
    "# Only, because its extremely large\n",
    "\n",
    "# Our first dataset which is a sample dataset taken from Kaggle and Sentiment140 and its used for highly complex classification\n",
    "# algorithms that take forever to excute in large datasets.This sample was taken with an equal distribution of the positive \n",
    "# and negative sentiments and will be used in all classifiers \n",
    "\n",
    "data =  pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\data1.csv', skiprows=0, low_memory=False) \n",
    "# Shuffle the rows of the dataset \n",
    "data = shuffle(data)\n",
    "\n",
    "\n",
    "# The dataset contains 1578612 tweets coming from two sources: Kaggle and Sentiment140. The Sentiment column correspond \n",
    "# to our label class taking a binary value, 0 if the tweet is negative, 1 if the tweet is positive.\n",
    "data2= pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\data2.csv', skiprows=0, low_memory=False) \n",
    "\n",
    "# Reindex the data frames and drop the column added by the reset_index function\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Set max_colwidth to 140 in order to fully see the tweet\n",
    "pd.set_option('max_colwidth', 140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 10 rows of the first dataset\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns in the first dataset\n",
    "data=data.drop(['Index'], axis = 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the first 10 rows of the second dataset \n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns in the Second dataset\n",
    "data2=data2.drop(['ItemID','SentimentSource'], axis = 1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can notice some important points inside the datasets.\n",
    "# 1) Acronyms for example: \"bf\" or more complicated \"APL\". Does it means apple ? Apple (the company) ? In this context we have \"friend\" after so we could think that he refers to his smartphone and so Apple, but what about if the word \"friend\" was not here ?\n",
    "# 2) The presence of sequences of repeated characters such as \"Juuuuuuuuuuuuuuuuussssst\"\n",
    "# 3) The presence of emoticons, \":O\", \"T_T\", \":-|\" and much more, give insights about user's moods.\n",
    "# 4) Spelling mistakes like \"im gunna\" or \"mi\".\n",
    "# 5) The precence of nouns such as \"TV\", \"New Moon\".\n",
    "# 6) People also indicate moods, emotions, states, between two such as, \\cries*, *hummin*, *sigh*.\n",
    "# 7) The negation, can't, cannot, don't, haven't that we need to handle.\n",
    "\n",
    "# And so on. As you can see, it is extremely complex to deal with language and that's why Natural Language Processing where Sentiment Analysis is one of its subtopic is a hot topic and lot of problems are still not solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig, ax = plt.subplots()\n",
    "counts, bins, patches = ax.hist(data.Sentiment.as_matrix(), edgecolor='gray')\n",
    "\n",
    "# Set plot title\n",
    "ax.set_title(\"Histogram of Sentiments of dataset1\")\n",
    "\n",
    "# Set x-axis name\n",
    "ax.set_xlabel(\"Sentiment\")\n",
    "\n",
    "# Set y-axis name\n",
    "ax.set_ylabel(\"Frequecy\")\n",
    "\n",
    "# Select the first patch (a rectangle, object of class matplotlib.patches.Patch)\n",
    "# corresponding to negative sentiment and color it\n",
    "patches[0].set_facecolor(\"#5d4037\")\n",
    "patches[0].set_label(\"negative\")\n",
    "\n",
    "# Same for the positive sentiment but in another color.\n",
    "patches[-1].set_facecolor(\"#ff9100\")\n",
    "patches[-1].set_label(\"positive\")\n",
    "\n",
    "# Add legend to a plot     \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was our first balanced dataset that was edited and used as a sample from the Sentiment140 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig, ax = plt.subplots()\n",
    "counts, bins, patches = ax.hist(data2.Sentiment.as_matrix(), edgecolor='gray')\n",
    "\n",
    "# Set plot title\n",
    "ax.set_title(\"Histogram of Sentiments of dataset2\")\n",
    "\n",
    "# Set x-axis name\n",
    "ax.set_xlabel(\"Sentiment\")\n",
    "\n",
    "# Set y-axis name\n",
    "ax.set_ylabel(\"Frequecy\")\n",
    "\n",
    "# Select the first patch (a rectangle, object of class matplotlib.patches.Patch)\n",
    "# corresponding to negative sentiment and color it\n",
    "patches[0].set_facecolor(\"#5d4037\")\n",
    "patches[0].set_label(\"negative\")\n",
    "\n",
    "# Same for the positive sentiment but in another color.\n",
    "patches[-1].set_facecolor(\"#ff9100\")\n",
    "patches[-1].set_label(\"positive\")\n",
    "\n",
    "# Add legend to a plot     \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second dataset also seems to be really well-balanced between negative and positive sentiment, let's confirm that by displying numeric values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "data.Sentiment.value_counts()\n",
    "# Count of tweets corresponding to the positive and negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2\n",
    "data2.Sentiment.value_counts()\n",
    "# Count of tweets corresponding to the positive and negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is important to check if we have duplicates in tweets which is something that arise very often because of the RT (Retweet),\n",
    "# Show duplicated tweets if exist in Dataset1\n",
    "len(data[data.duplicated('SentimentText')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show duplicated tweets if exist in Dataset 2\n",
    "len(data2[data2.duplicated('SentimentText')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of RT in the first dataset\n",
    "CountofRT = data['SentimentText'].str.contains('RT').value_counts()\n",
    "CountofRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of RT in the second dataset\n",
    "CountofRT2 = data2['SentimentText'].str.contains('RT').value_counts()\n",
    "CountofRT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thats a very good news to have some retweets in our dataset but with zero duplications and thats a very \n",
    "# good thing when it comes to training our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "\n",
    "# To have a good preprocessing to our data, we will be using some beneficial resources\n",
    "\n",
    "# emoticon dictionary regrouping 132 of the most used emoticons in western with their sentiment, negative or positive.\n",
    "# An acronym dictionary of 5465 acronyms with their translation\n",
    "# A stop word dictionary corresponding to words which are filtered out before or after processing of natural language data because they are not useful in our case.\n",
    "# A positive and negative word dictionaries.\n",
    "# A negative contractions and auxiliaries dictionary which will be used to detect negation in a given tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Smileys Dataset\n",
    "emoticons = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Smileys.csv', skiprows=0, low_memory=False)\n",
    "positive_emoticons = emoticons[emoticons.Sentiment == 1]\n",
    "negative_emoticons = emoticons[emoticons.Sentiment == 0]\n",
    "emoticons.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Acronyms Dataset\n",
    "acronyms = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Acronyms.csv', skiprows=0, low_memory=False)\n",
    "acronyms.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stopwords Dataset\n",
    "stops = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Stopwords.csv', skiprows=0, low_memory=False)\n",
    "stops.columns = ['Word']\n",
    "stops.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resources showed above are mainly used only for the preprocessing part. \n",
    "\n",
    "\n",
    "# Another resource that we are going to use is a lexicon which corresponds to a list of words where each word is associated with its polarity, positive or negative.\n",
    "# The lexicon is divided into two distinct files, one for positive words, containing 2005 entries and the other for negative words containing 4782 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Positive Words Dataset\n",
    "positive_words = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Positive-words.csv', skiprows=0, low_memory=False ,sep='\\t')\n",
    "positive_words.columns = ['Word', 'Sentiment']\n",
    "positive_words.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Negative Words Dataset\n",
    "negative_words = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Negative-words.csv', skiprows=0, low_memory=False ,sep='\\t',encoding= 'unicode_escape')\n",
    "negative_words.columns = ['Word', 'Sentiment']\n",
    "negative_words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Negations Dataset\n",
    "negation_words = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Negation.csv', skiprows=0, low_memory=False)\n",
    "negation_words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "# One of the most important parts that is going to be crucial for the learning part is the preprocessing of the data. Indeed as they are, we can't just use a learning algorithm because the given result would be highly biased due to the inconsistency of the data.\n",
    "\n",
    "# To do this we are going to pass our data through these different steps:\n",
    "\n",
    "# 1) Replace all emoticons by their sentiment polarity ||pos||/||neg|| using the emoticon dictionary.\n",
    "# 2) Replace all URLs with a tag ||url||.\n",
    "# 3) Remove Unicode characters.\n",
    "# 4) Decode HTML entities.\n",
    "# 5) Reduce all letters to lowercase (We should take care of proper nouns but for simplicity we will lower them as well) (After emoticons because they can use upper case letters)\n",
    "# 6) Replace all usernames/targets @ with ||target||.\n",
    "# 7) Replace all acronyms with their translation.\n",
    "# 8) Replace all negations (e.g: not, no, never) by tag ||not||.\n",
    "# 9) Replace a sequence of repeated characters by two characters (e.g: \"helloooo\" = \"helloo\") to keep the emphasized usage of the word.\n",
    "\n",
    "# Not to forget the imporatnce of the tagging and lexicons phases inside the preprocessing we are doing here.\n",
    "# In addition, those two are extremely important in terms of NLP and Sentiment Analysis.\n",
    "\n",
    "\n",
    "# All these techniques will help us reach better results in the validation and testing phases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regular Expressions\n",
    "\n",
    "# Creating the functions to detect and replace emoticons for the first dataset\n",
    "\n",
    "def make_emoticon_pattern(emoticons):\n",
    "    pattern = \"|\".join(map(re.escape, emoticons.Smiley))\n",
    "    pattern = \"(?<=\\s)(\" + pattern + \")(?=\\s)\"\n",
    "    return pattern\n",
    "\n",
    "def find_with_pattern(pattern, replace=False, tag=None):\n",
    "    if replace and tag == None:\n",
    "        raise Exception(\"Parameter error\", \"If replace=True you should add the tag by which the pattern will be replaced\")\n",
    "    regex = re.compile(pattern)\n",
    "    if replace:\n",
    "        return data.SentimentText.apply(lambda tweet: re.sub(pattern, tag, \" \" + tweet + \" \"))\n",
    "    return data.SentimentText.apply(lambda tweet: re.findall(pattern, \" \" + tweet + \" \"))\n",
    "\n",
    "# Applying the functions by relying on the positive and negative emoticons retrieved from the resources\n",
    "pos_emoticons_found = find_with_pattern(make_emoticon_pattern(positive_emoticons))\n",
    "neg_emoticons_found = find_with_pattern(make_emoticon_pattern(negative_emoticons))\n",
    "\n",
    "# Searching through the dataset to find the number of positive and negative emoticons presented\n",
    "nb_pos_emoticons = len(pos_emoticons_found[pos_emoticons_found.map(lambda emoticons : len(emoticons) > 0)])\n",
    "nb_neg_emoticons = len(neg_emoticons_found[neg_emoticons_found.map(lambda emoticons : len(emoticons) > 0)])\n",
    "print (\"Number of positive emoticons in the first dataset: \" + str(nb_pos_emoticons))\n",
    "print(\"Number of negative emoticons in the first dataset: \" + str(nb_neg_emoticons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the replacement function to the first dataset\n",
    "data.SentimentText = find_with_pattern(make_emoticon_pattern(positive_emoticons), True, '||pos||')\n",
    "data.SentimentText = find_with_pattern(make_emoticon_pattern(negative_emoticons), True, '||neg||')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same procedure for the second dataset\n",
    "def make_emoticon_pattern2(emoticons):\n",
    "    pattern = \"|\".join(map(re.escape, emoticons.Smiley))\n",
    "    pattern = \"(?<=\\s)(\" + pattern + \")(?=\\s)\"\n",
    "    return pattern\n",
    "\n",
    "def find_with_pattern2(pattern, replace=False, tag=None):\n",
    "    if replace and tag == None:\n",
    "        raise Exception(\"Parameter error\", \"If replace=True you should add the tag by which the pattern will be replaced\")\n",
    "    regex = re.compile(pattern)\n",
    "    if replace:\n",
    "        return data2.SentimentText.apply(lambda tweet: re.sub(pattern, tag, \" \" + tweet + \" \"))\n",
    "    return data2.SentimentText.apply(lambda tweet: re.findall(pattern, \" \" + tweet + \" \"))\n",
    "\n",
    "pos_emoticons_found = find_with_pattern2(make_emoticon_pattern2(positive_emoticons))\n",
    "neg_emoticons_found = find_with_pattern2(make_emoticon_pattern2(negative_emoticons))\n",
    "\n",
    "nb_pos_emoticons = len(pos_emoticons_found[pos_emoticons_found.map(lambda emoticons : len(emoticons) > 0)])\n",
    "nb_neg_emoticons = len(neg_emoticons_found[neg_emoticons_found.map(lambda emoticons : len(emoticons) > 0)])\n",
    "print (\"Number of positive emoticons in the second dataset: \" + str(nb_pos_emoticons))\n",
    "print(\"Number of negative emoticons in the second dataset: \" + str(nb_neg_emoticons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the replacement function to the second dataset\n",
    "\n",
    "\n",
    "data2.SentimentText = find_with_pattern2(make_emoticon_pattern2(positive_emoticons), True, '||pos||')\n",
    "data2.SentimentText = find_with_pattern2(make_emoticon_pattern2(negative_emoticons), True, '||neg||')\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the URLS\n",
    "\n",
    "\n",
    "# Using the same method as for emoticons, we find all urls in each tweet and replace them by the tag ||url||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing URLS\n",
    "\n",
    "# First Dataset \n",
    "pattern_url = re.compile(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "url_found = find_with_pattern(pattern_url)\n",
    "print (\"Number of urls of the first dataset: \" + str(len(url_found[url_found.map(lambda urls : len(urls) > 0)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Dataset\n",
    "pattern_url2 = re.compile(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "url_found2 = find_with_pattern2(pattern_url2)\n",
    "print (\"Number of urls of the second dataset: \" + str(len(url_found2[url_found2.map(lambda urls : len(urls) > 0)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some URL Examples before preprocessing\n",
    "data2[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all URLS with ||url|| Expression for both datasets\n",
    "data.SentimentText = find_with_pattern(pattern_url, True, '||url||')\n",
    "data2.SentimentText = find_with_pattern2(pattern_url2, True, '||url||')\n",
    "# URLS after preprocessing\n",
    "data2[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove unicode characteres since they can cause problems during the tokenization process. We keep only ASCII characteres.\n",
    "\n",
    "\n",
    "# Some examples of unicode character before preprocessing of dataset2\n",
    "data2[1578592:1578602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Unicode characters keeping asci\n",
    "\n",
    "\n",
    "# If you are using python 3 you should use both functions\n",
    "def remove_unicode(text):\n",
    "    try:\n",
    "        text = text.encode('ascii','ignore')\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def remove_unicode2(text):\n",
    "    try:\n",
    "        text = text.decode('unicode_escape')\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "# Removing unicode character of both datasets\n",
    "data.SentimentText = data.SentimentText.apply(lambda tweet: remove_unicode(tweet))\n",
    "data2.SentimentText = data2.SentimentText.apply(lambda tweet: remove_unicode(tweet))\n",
    "data.SentimentText = data.SentimentText.apply(lambda tweet: remove_unicode2(tweet))\n",
    "data2.SentimentText = data2.SentimentText.apply(lambda tweet: remove_unicode2(tweet))\n",
    "# After removing Unicode characters\n",
    "data2[1578592:1578602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Decoding HTML entities\n",
    "data2.SentimentText[599982]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply decode HTML entities\n",
    "import html\n",
    "# Convert tweets in unicode utf-8 to avoid mixing unicode with ascii and causing an error during unescape\n",
    "data.SentimentText  = data.SentimentText.apply(lambda tweet: html.unescape(str(tweet)))\n",
    "data2.SentimentText  = data2.SentimentText.apply(lambda tweet: html.unescape(str(tweet)))\n",
    "# After decoding HTML entities\n",
    "data2.SentimentText[599982]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing all letters to lower case \n",
    "\n",
    "#This is part is extremely simple, we just transform all tweets to lower case in order to make easier the next operations with the acronym and stop dictionaries and more generally, to make easier comparisons. We should take care of proper noun but for simplicity we skip this.\n",
    "\n",
    "# Example before doing the lower case function\n",
    "data2.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce all letters to lower case\n",
    "data.SentimentText = data.SentimentText.str.lower()\n",
    "data2.SentimentText = data2.SentimentText.str.lower()\n",
    "\n",
    "# After preprocessing of letters\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all usernames @ with the tag ||target||\n",
    "\n",
    "\n",
    "# Since we don't need to take into account usernames in order to determine the sentiment of a tweet we replace them by the tag ||target||.\n",
    "\n",
    "# Before replacement\n",
    "data2[45:55]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to check how many mentions we have in both datasets\n",
    "\n",
    "# NLP expression that is related to all string values that are @ followed by any names or string values\n",
    "pattern_usernames = \"@\\w{1,}\"\n",
    "usernames_found = find_with_pattern(pattern_usernames)\n",
    "usernames_found2 = find_with_pattern2(pattern_usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of targets in the first dataset\n",
    "len(data.SentimentText[usernames_found.apply(lambda usernames : len(usernames) > 0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of targets in the second dataset\n",
    "len(data2.SentimentText[usernames_found2.apply(lambda usernames : len(usernames) > 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all usernames/targets @ with the tag ||target||\n",
    "data.SentimentText = find_with_pattern(pattern_usernames, True, '||target||')\n",
    "data2.SentimentText = find_with_pattern2(pattern_usernames, True, '||target||')\n",
    "\n",
    "# Sample rows after replacement\n",
    "data2[45:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7) Replace all acronyms with their translation\n",
    "\n",
    "#Next, we replace all acronyms with their translation using the acronym dictionary.\n",
    "#At this point, tweets are going to be tokenized by getting rid of the punctuation and using split in order to do the process really fast. We could use nltk.tokenizer but it is definitly much much slower (also much more accurate but its not a severe problem).\n",
    "#Even though replacements will not be perfect, a simple example is the acronym \"im\" meaning \"instant message\". It would not be surprising that in most of the cases, \"im\" means \"I am\". For that, some improvements will be done later on to enhance our results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a dictionary of acronym which will be used to get translations\n",
    "acronym_dictionary = dict(zip(acronyms.Acronym, acronyms.Translation))\n",
    "\n",
    "# Will be used to get rid of the punctuation in tweets (does not include | since we use it for our tokens and ' \n",
    "# to take care of don't, can't)\n",
    "punctuation = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{}~'\n",
    "\n",
    "# Frequency table for acronyms\n",
    "acronyms_counter = Counter()\n",
    "\n",
    "# Loop on acronyms to replace those matched in the tweet by the corresponding translations\n",
    "# Return the tweet and the acronyms used\n",
    "def acronym_to_translation(tweet, acronyms_counter):\n",
    "    table = str.maketrans(punctuation,\" \" * len(punctuation))\n",
    "    tweet = str(tweet).translate(table)\n",
    "    words = tweet.split()\n",
    "    new_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        if acronym_dictionary.__contains__(word):\n",
    "            acronyms_counter[word] += 1\n",
    "            new_words.extend(acronym_dictionary[word].split())\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "data.SentimentText = data.SentimentText.apply(lambda tweet: acronym_to_translation(str(tweet), acronyms_counter))\n",
    "\n",
    "# Get and display top20 acronyms of the first dataset\n",
    "top20acronyms = acronyms_counter.most_common(20)\n",
    "top20acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same process for the second dataset\n",
    "\n",
    "# Create a dictionary of acronym which will be used to get translations\n",
    "acronym_dictionary = dict(zip(acronyms.Acronym, acronyms.Translation))\n",
    "\n",
    "# Will be used to get rid of the punctuation in tweets (does not include | since we use it for our tokens and ' \n",
    "# to take care of don't, can't)\n",
    "punctuation = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{}~'\n",
    "\n",
    "# Frequency table for acronyms\n",
    "acronyms_counter2 = Counter()\n",
    "\n",
    "# Loop on acronyms to replace those matched in the tweet by the corresponding translations\n",
    "# Return the tweet and the acronyms used\n",
    "def acronym_to_translation2(tweet, acronyms_counter2):\n",
    "    table = str.maketrans(punctuation,\" \" * len(punctuation))\n",
    "    tweet = str(tweet).translate(table)\n",
    "    words = tweet.split()\n",
    "    new_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        if acronym_dictionary.__contains__(word):\n",
    "            acronyms_counter2[word] += 1\n",
    "            new_words.extend(acronym_dictionary[word].split())\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "data2.SentimentText = data2.SentimentText.apply(lambda tweet: acronym_to_translation2(str(tweet ), acronyms_counter2))\n",
    "\n",
    "# Get and display top20 acronyms of the second dataset\n",
    "top20acronyms2 = acronyms_counter2.most_common(20)\n",
    "top20acronyms2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to better visualize the top 20 acronym\n",
    "print(\"Dataset 1:\")\n",
    "for i, (acronym, value) in enumerate(top20acronyms):\n",
    "    print (str(i + 1) + \") \" + acronym + \" => \" + acronym_dictionary[acronym] + \" : \" + str(value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to better visualize the top 20 acronym\n",
    "print('Dataset 2:')\n",
    "for i, (acronym, value) in enumerate(top20acronyms2):\n",
    "    print (str(i + 1) + \") \" + acronym + \" => \" + acronym_dictionary[acronym] + \" : \" + str(value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a bar plot\n",
    "plt.close()\n",
    "top20acronym_keys = [x[0] for x in top20acronyms]\n",
    "top20acronym_values = [x[1] for x in top20acronyms]\n",
    "indexes = np.arange(len(top20acronym_keys))\n",
    "width = 0.7\n",
    "plt.bar(indexes, top20acronym_values, width)\n",
    "plt.title('Top 20 acronyms in Dataset 1')\n",
    "plt.xticks(indexes + width * 0.5, top20acronym_keys, rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a bar plot\n",
    "plt.close()\n",
    "top20acronym_keys2 = [x[0] for x in top20acronyms2]\n",
    "top20acronym_values2 = [x[1] for x in top20acronyms2]\n",
    "indexes = np.arange(len(top20acronym_keys2))\n",
    "width = 0.7\n",
    "plt.bar(indexes, top20acronym_values2, width)\n",
    "plt.title('Top 20 acronyms in Dataset 2')\n",
    "plt.xticks(indexes + width * 0.5, top20acronym_keys2, rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all negations by tag ||not||\n",
    "\n",
    "#We replace all negations such as not, no, don't and so on, using the negation dictionary in order to take more or less of sentences like \"I don't like it\". Here like should not be considered as positive because of the \"don't\" before. To do so we will replace \"don't\" by ||not|| and the word like will not be counted as positive.\n",
    "#In general, each time a negation is encountered, the words followed by the negation word contained in the positive and negative word dictionaries will be reversed, positive becomes negative, negative becomes positive, we will do this when we will try to find positive and negative words..\n",
    "# Since we replaced the negations and the positive and negative words, the tagging will reduce the harm due to the combination of both\n",
    "\n",
    "# Before replacement\n",
    "print (data2.SentimentText[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataframe into a dictionary\n",
    "negation_dictionary = dict(zip(negation_words.Negation, negation_words.Tag))\n",
    "\n",
    "# Find a negation in a tweet and replace it by its tag\n",
    "def replace_negation(tweet):\n",
    "    return [negation_dictionary[word] if negation_dictionary.__contains__(word) else word for word in tweet]\n",
    "    \n",
    "# Apply the function on every tweet\n",
    "data.SentimentText = data.SentimentText.apply(lambda tweet: replace_negation(tweet))\n",
    "data2.SentimentText = data2.SentimentText.apply(lambda tweet: replace_negation(tweet))\n",
    "# After replacement\n",
    "print (data2.SentimentText[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace a sequence of repeated characters by two caracters\n",
    "\n",
    "#There are many words containing repeated sequences of charaters usually used to emphasize a word.\n",
    "#We are going to reduce the number of repeated charaters in order to potentially reduce the feature space (the words in our case) and keep their emphasized aspect\n",
    "\n",
    "# Sample data before removing repeated characters\n",
    "data2[1578604:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace a sequence of repeated characters by two characters\n",
    "\n",
    "# We use the corresponding regular expression to detected repeated characters inside a word\n",
    "pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "def reduce_sequence_word(word):\n",
    "    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n",
    "\n",
    "def reduce_sequence_tweet(tweet):\n",
    "    return [reduce_sequence_word(word) for word in tweet]\n",
    "\n",
    "# Applying the replacements functions to both datasets\n",
    "data.SentimentText = data.SentimentText.apply(lambda tweet: reduce_sequence_tweet(tweet))\n",
    "data2.SentimentText = data2.SentimentText.apply(lambda tweet: reduce_sequence_tweet(tweet))\n",
    "\n",
    "# After removing repeated characters\n",
    "data2[1578604:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Dataset\n",
    "def make_training_test_sets(data):\n",
    "    \n",
    "    # Before making the training and test set, we shuffle our data set in order to avoid keeping any order\n",
    "    data_shuffled = data.iloc[np.random.permutation(len(data))]\n",
    "    data = data_shuffled.reset_index(drop=True)\n",
    "\n",
    "    # Join the words back into one string separated by space for each tweet\n",
    "    data.SentimentText = data.SentimentText.apply(lambda tweet: \" \".join(tweet))\n",
    "\n",
    "    # Separate positive and negative tweets\n",
    "    positive_tweets = data[data.Sentiment == 1]\n",
    "    negative_tweets = data[data.Sentiment == 0]\n",
    "\n",
    "    # Cutoff, 9/10 for training of each sentiment and 1/10 of each sentiment for testing\n",
    "    positive_tweets_cutoff = int(len(positive_tweets) * (9./10.))\n",
    "    negative_tweets_cutoff = int(len(negative_tweets) * (9./10.))\n",
    "\n",
    "    # Make the training and test set\n",
    "    training_tweets = pd.concat([positive_tweets[:positive_tweets_cutoff], negative_tweets[:negative_tweets_cutoff]])\n",
    "    test_tweets = pd.concat([positive_tweets[positive_tweets_cutoff:], negative_tweets[negative_tweets_cutoff:]])\n",
    "\n",
    "    # We suffle the training and test set to break the order of tweets based on their sentiment\n",
    "    training_tweets = training_tweets.iloc[np.random.permutation(len(training_tweets))].reset_index(drop=True)\n",
    "    test_tweets = test_tweets.iloc[np.random.permutation(len(test_tweets))].reset_index(drop=True)\n",
    "    \n",
    "    return training_tweets, test_tweets\n",
    "\n",
    "training_tweets, test_tweets = make_training_test_sets(data)\n",
    "\n",
    "print (\"size of training set of first dataset: \" + str(len(training_tweets)))\n",
    "print (\"size of test set of first dataset: \" + str(len(test_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Dataset\n",
    "def make_training_test_sets2(data2):\n",
    "    \n",
    "    # Before making the training and test set, we shuffle our data set in order to avoid keeping any order\n",
    "    data_shuffled2 = data2.iloc[np.random.permutation(len(data2))]\n",
    "    data2 =  data_shuffled2.reset_index(drop=True)\n",
    "\n",
    "    # Join the words back into one string separated by space for each tweet\n",
    "    data2.SentimentText = data2.SentimentText.apply(lambda tweet: \" \".join(tweet))\n",
    "\n",
    "    # Separate positive and negative tweets\n",
    "    positive_tweets = data2[data2.Sentiment == 1]\n",
    "    negative_tweets = data2[data2.Sentiment == 0]\n",
    "\n",
    "    # Cutoff, 3/4 for training of each sentiment and 1/4 of each sentiment for testing\n",
    "    positive_tweets_cutoff = int(len(positive_tweets) * (3./4.))\n",
    "    negative_tweets_cutoff = int(len(negative_tweets) * (3./4.))\n",
    "\n",
    "    # Make the training and test set\n",
    "    training_tweets2 = pd.concat([positive_tweets[:positive_tweets_cutoff], negative_tweets[:negative_tweets_cutoff]])\n",
    "    test_tweets2 = pd.concat([positive_tweets[positive_tweets_cutoff:], negative_tweets[negative_tweets_cutoff:]])\n",
    "\n",
    "    # We suffle the training and test set to break the order of tweets based on their sentiment\n",
    "    training_tweets2 = training_tweets2.iloc[np.random.permutation(len(training_tweets2))].reset_index(drop=True)\n",
    "    test_tweets2 = test_tweets2.iloc[np.random.permutation(len(test_tweets2))].reset_index(drop=True)\n",
    "    \n",
    "    return training_tweets2, test_tweets2\n",
    "\n",
    "training_tweets2, test_tweets2 = make_training_test_sets2(data2)\n",
    "\n",
    "print (\"size of training set of second dataset: \" + str(len(training_tweets2)))\n",
    "print (\"size of test set of second dataset: \" + str(len(test_tweets2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will start training and validating the data of dataset1 unigram\n",
    "\n",
    "# We will use Sklearn for kfolding, calculating the metrics: precision, accuracy, recall, f1 score and confusion matrix\n",
    "# We will be using the famous classification algorithms Multinomial Naive Bayes, KNeighbor, SGD, Logistic regression, Decision tree, and SVM\n",
    "# we import pickle to save the models as a pickle file, to be used later on for testing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# Starting with dataset one we will perform all the algorithms before removing stop words and before stemming, and using unigrams only\n",
    "def classify_unigram(training_tweets, test_tweets, ngram=(1, 1)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_uni_before_ss', MultinomialNB()))\n",
    "    models.append(('DT_uni_before_ss', DecisionTreeClassifier()))\n",
    "    models.append(('KN_uni_before_ss', KNeighborsClassifier()))\n",
    "    models.append(('SGD_uni_before_ss', SGDClassifier()))\n",
    "    models.append(('SVM_uni_before_ss', SVC()))\n",
    "    models.append(('LR_uni_before_ss', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory as a pkl file to be used later in testing\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                # if loop to store the fold with the best F1 score\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        # saving the classifier with the countvectorizer function to be used later on\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score        \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "        \n",
    "\n",
    "        \n",
    "classify_unigram(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same procedure for the second dataset using only Multi NB and unigrams\n",
    "def classify_unigram2(training_tweets2, test_tweets2, ngram=(1, 1)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_uni_before_ss2', MultinomialNB()))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets2):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                    \n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets2)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "        \n",
    "classify_unigram2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now its unigrams and bigrams of dataset1\n",
    "def classify_unigrambigram(training_tweets, test_tweets, ngram=(1, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_unibi_before_ss', MultinomialNB()))\n",
    "    models.append(('DT_unibi_before_ss', DecisionTreeClassifier()))\n",
    "    models.append(('KN_unibi_before_ss', KNeighborsClassifier()))\n",
    "    models.append(('SGD_unibi_before_ss', SGDClassifier()))\n",
    "    models.append(('SVM_unibi_before_ss', SVC()))\n",
    "    models.append(('LR_unibi_before_ss', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "               \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigrambigram(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram and bigram usage for dataset2 \n",
    "def classify_unigrambigram2(training_tweets2, test_tweets2, ngram=(1, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_unibi_before_ss2', MultinomialNB()))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets2):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                    \n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets2)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "        \n",
    "classify_unigrambigram2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bigrams only for dataset1\n",
    "def classify_bigram(training_tweets, test_tweets, ngram=(2, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_bi_before_ss', MultinomialNB()))\n",
    "    models.append(('DT_bi_before_ss', DecisionTreeClassifier()))\n",
    "    models.append(('KN_bi_before_ss', KNeighborsClassifier()))\n",
    "    models.append(('SGD_bi_before_ss', SGDClassifier()))\n",
    "    models.append(('SVM_bi_before_ss', SVC()))\n",
    "    models.append(('LR_bi_before_ss', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_bigram(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bigrams only for dataset2\n",
    "def classify_bigram2(training_tweets2, test_tweets2, ngram=(2, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_bi_before_ss2', MultinomialNB()))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets2):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                    \n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets2)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "        \n",
    "classify_bigram2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a word frequency table to see which words are the most used\n",
    "word_frequency_table = Counter()\n",
    "\n",
    "def count_word(tweet):\n",
    "    for word in tweet:\n",
    "        word_frequency_table[word] += 1\n",
    "    return tweet\n",
    "# First dataset\n",
    "data.SentimentText.map(lambda tweet: count_word(tweet))\n",
    "print('Most frequent words in dataset 1:')\n",
    "word_frequency_table.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a word frequency table to see which words are the most used\n",
    "word_frequency_table2 = Counter()\n",
    "\n",
    "def count_word2(tweet):\n",
    "    for word in tweet:\n",
    "        word_frequency_table2[word] += 1\n",
    "    return tweet\n",
    "# Second dataset\n",
    "data2.SentimentText.map(lambda tweet: count_word2(tweet))\n",
    "print('Most frequent words in dataset 2:')\n",
    "word_frequency_table2.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # list of tags\n",
    "tags = ['||target||', '||url||', '||pos||', '||neg||', '||not||']\n",
    "\n",
    "# list of tuples representing tags with their corresponding count in dataset1\n",
    "tag_counter = [(w, c) for w,c in word_frequency_table.items() if w in tags]\n",
    "print('Tag counter in dataset 1: ')\n",
    "print (tag_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # list of tags\n",
    "tags = ['||target||', '||url||', '||pos||', '||neg||', '||not||']\n",
    "\n",
    "# list of tuples representing tags with their corresponding count in dataset2\n",
    "tag_counter2 = [(w, c) for w,c in word_frequency_table2.items() if w in tags]\n",
    "print('Tag counter in dataset 2: ')\n",
    "print (tag_counter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "tag_counter_keys = [x[0] for x in tag_counter]\n",
    "tag_counter_values = [x[1] for x in tag_counter]\n",
    "indexes = np.arange(len(tag_counter_keys))\n",
    "width = 0.7\n",
    "plt.bar(indexes, tag_counter_values, width)\n",
    "plt.title(\"Counts in Dataset1\")\n",
    "plt.xticks(indexes + width * 0.5, tag_counter_keys, rotation=\"vertical\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "tag_counter_keys2 = [x[0] for x in tag_counter2]\n",
    "tag_counter_values2 = [x[1] for x in tag_counter2]\n",
    "indexes = np.arange(len(tag_counter_keys2))\n",
    "width = 0.7\n",
    "plt.bar(indexes, tag_counter_values2, width)\n",
    "plt.title(\"Counts in Dataset2\")\n",
    "plt.xticks(indexes + width * 0.5, tag_counter_keys2, rotation=\"vertical\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataframe into a dictionary\n",
    "stopword_dictionary = dict.fromkeys(stops.Word, None)\n",
    "\n",
    "# Remove stopword from tweets\n",
    "def remove_stopwords(tweet):\n",
    "    tweet = [stopword_dictionary[word] if stopword_dictionary.__contains__(word) else word for word in tweet]\n",
    "    return [word for word in tweet if word]\n",
    "# Remove stop words in both dataset1 and 2\n",
    "data.SentimentText = data.SentimentText.apply(lambda tweet: remove_stopwords(tweet))\n",
    "data2.SentimentText = data2.SentimentText.apply(lambda tweet: remove_stopwords(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words after deleting stop words\n",
    "word_frequency_table = Counter()\n",
    "\n",
    "# Dataset1\n",
    "data.SentimentText.map(lambda tweet: count_word(tweet))\n",
    "print('Most common words after deleting stop words in dataset1: ')\n",
    "print (word_frequency_table.most_common()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words after deleting stop words\n",
    "word_frequency_table2 = Counter()\n",
    "\n",
    "#Dataset2\n",
    "data2.SentimentText.map(lambda tweet: count_word2(tweet))\n",
    "print('Most common words after deleting stop words in dataset2: ')\n",
    "print (word_frequency_table2.most_common()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same procedure after removing stopwords\n",
    "# unigrams only after removing stop words for dataset1\n",
    "def classify_unigram_nostop(training_tweets, test_tweets, ngram=(1, 1)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_uni_nostop', MultinomialNB()))\n",
    "    models.append(('DT_uni_nostop', DecisionTreeClassifier()))\n",
    "    models.append(('KN_uni_nostop', KNeighborsClassifier()))\n",
    "    models.append(('SGD_uni_nostop', SGDClassifier()))\n",
    "    models.append(('SVM_uni_nostop', SVC()))\n",
    "    models.append(('LR_uni_nostop', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigram_nostop(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset2 no stop words and unigrams only\n",
    "def classify_unigram_nostop2(training_tweets2, test_tweets2, ngram=(1, 1)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_uni_nostop2', MultinomialNB()))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets2)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigram_nostop2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset1 nostopwords with unigrams and bigrams\n",
    "def classify_unigrambigram_nostop(training_tweets, test_tweets, ngram=(1, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_unibi_nostop', MultinomialNB()))\n",
    "    models.append(('DT_unibi_nostop', DecisionTreeClassifier()))\n",
    "    models.append(('KN_unibi_nostop', KNeighborsClassifier()))\n",
    "    models.append(('SGD_unibi_nostop', SGDClassifier()))\n",
    "    models.append(('SVM_unibi_nostop', SVC()))\n",
    "    models.append(('LR_unibi_nostop', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigrambigram_nostop(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset2 for unigrams and bigrams after removing stopwords\n",
    "def classify_unigrambigram_nostop2(training_tweets2, test_tweets2, ngram=(1, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_unibi_nostop2', MultinomialNB()))\n",
    "\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets2):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets2)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigrambigram_nostop2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset1 bigrams only after removing stopwords\n",
    "def classify_bigram_nostop(training_tweets, test_tweets, ngram=(2, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_bi_nostop', MultinomialNB()))\n",
    "    models.append(('DT_bi_nostop', DecisionTreeClassifier()))\n",
    "    models.append(('KN_bi_nostop', KNeighborsClassifier()))\n",
    "    models.append(('SGD_bi_nostop', SGDClassifier()))\n",
    "    models.append(('SVM_bi_nostop', SVC()))\n",
    "    models.append(('LR_bi_nostop', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_bigram_nostop(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset2 after removing stopwords and using only bigrams\n",
    "def classify_bigram_nostop2(training_tweets2, test_tweets2, ngram=(2, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_bi_nostop2', MultinomialNB()))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets2):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets2)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_bigram_nostop2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Stemming\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use porterstemmer for stemming to check whether our scores will improve or not\n",
    "import nltk\n",
    "\n",
    "pstemmer = nltk.PorterStemmer()\n",
    "def stemming_words(tweet):\n",
    "    return [pstemmer.stem(word) if word not in tags else word for word in tweet]\n",
    "\n",
    "# applying the stemming function to both datasets\n",
    "data.SentimentText = data.SentimentText.apply(lambda tweet: stemming_words(tweet))\n",
    "data2.SentimentText = data2.SentimentText.apply(lambda tweet: stemming_words(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Stemming\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset1 unigrams only after stemming\n",
    "def classify_unigram_stem(training_tweets, test_tweets, ngram=(1, 1)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_uni_stem', MultinomialNB()))\n",
    "    models.append(('DT_uni_stem', DecisionTreeClassifier()))\n",
    "    models.append(('KN_uni_stem', KNeighborsClassifier()))\n",
    "    models.append(('SGD_uni_stem', SGDClassifier()))\n",
    "    models.append(('SVM_uni_stem', SVC()))\n",
    "    models.append(('LR_uni_stem', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                        \n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigram_stem(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset2 unigrams after stemming\n",
    "def classify_unigram_stem2(training_tweets2, test_tweets2, ngram=(1, 1)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_uni_stem2', MultinomialNB()))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets2):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets2)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigram_stem2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigrams and bigrams after stemming for dataset1\n",
    "def classify_unigrambigram_stem(training_tweets, test_tweets, ngram=(1, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_unibi_stem', MultinomialNB()))\n",
    "    models.append(('DT_unibi_stem', DecisionTreeClassifier()))\n",
    "    models.append(('KN_unibi_stem', KNeighborsClassifier()))\n",
    "    models.append(('SGD_unibi_stem', SGDClassifier()))\n",
    "    models.append(('SVM_unibi_stem', SVC()))\n",
    "    models.append(('LR_unibi_stem', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigrambigram_stem(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigrams and bigrams after stemming for Dataset2\n",
    "def classify_unigrambigram_stem2(training_tweets2, test_tweets2, ngram=(1, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_unibi_stem2', MultinomialNB()))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets2):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets2)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_unigrambigram_stem2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bigrams only for dataset1 after stemming\n",
    "def classify_bigram_stem(training_tweets, test_tweets, ngram=(2, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_bi_stem', MultinomialNB()))\n",
    "    models.append(('DT_bi_stem', DecisionTreeClassifier()))\n",
    "    models.append(('KN_bi_stem', KNeighborsClassifier()))\n",
    "    models.append(('SGD_bi_stem', SGDClassifier()))\n",
    "    models.append(('SVM_bi_stem', SVC()))\n",
    "    models.append(('LR_bi_stem', LogisticRegression(solver='lbfgs')))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_bigram_stem(training_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams only for dataset2 after stemming\n",
    "def classify_bigram_stem2(training_tweets2, test_tweets2, ngram=(2, 2)):\n",
    "    global models\n",
    "    models = []\n",
    "    models.append(('Multi_bi_stem2', MultinomialNB()))\n",
    "    results= []\n",
    "    names= []\n",
    "    # F1 scores for each fold\n",
    "    global scores\n",
    "    scores = []\n",
    "    \n",
    "    #for name,model in models:\n",
    "        \n",
    "    # Provides train/test indices to split data in train, validation sets.\n",
    "    k_fold = KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "    # Used to convert a collection of text docuements to a matrix of token counts => Bag of words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "\n",
    "    # Confusion matrix with TP/FP/TN/FN\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for name,model in models:\n",
    "        print(name)\n",
    "        score2=0\n",
    "        for training_indices, validation_indices in k_fold.split(training_tweets2):\n",
    "                \n",
    "                training_features = count_vectorizer.fit_transform(training_tweets2.iloc[training_indices]['SentimentText'].values)\n",
    "                training_labels = training_tweets2.iloc[training_indices]['Sentiment'].values\n",
    "\n",
    "                validation_features = count_vectorizer.transform(training_tweets2.iloc[validation_indices]['SentimentText'].values)\n",
    "                validation_labels = training_tweets2.iloc[validation_indices]['Sentiment'].values\n",
    "\n",
    "                classifier = model\n",
    "                classifier.fit(training_features, training_labels)\n",
    "                validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "                confusion += confusion_matrix(validation_labels, validation_predictions)\n",
    "                score = f1_score(validation_labels, validation_predictions)\n",
    "                precision=precision_score(validation_labels, validation_predictions)\n",
    "                recall=recall_score(validation_labels,validation_predictions)\n",
    "                accuracy=accuracy_score(validation_labels, validation_predictions)\n",
    "                scores.append(score)\n",
    "                # Save to file in the current working directory\n",
    "                pkl_filename = \"C:/Users/USER/Desktop/BE/Models/\"+name+\".pkl\"\n",
    "                if score>score2:\n",
    "                    with open(pkl_filename, 'wb') as file:\n",
    "                        pickle.dump(classifier, file)\n",
    "                        pickle.dump(count_vectorizer, file)\n",
    "                    score2=score\n",
    "                \n",
    "        print('\\n')\n",
    "        print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "        print('\\n')\n",
    "        print ('F1 Score: ' +  (str(sum(scores)/len(scores)*100)+'%'))\n",
    "        print ('Accuracy: ' + str(accuracy*100)+ '%')\n",
    "        print ('Precision: ' + str(precision*100)+ '%')\n",
    "        print ('Recall: ' + str(recall*100)+ '%')\n",
    "        print('\\n')\n",
    "        print ('Confusion matrix:')\n",
    "        print (confusion)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "classify_bigram_stem2(training_tweets2, test_tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libaries \n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "\n",
    "# Load Smileys Dataset\n",
    "emoticons = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Smileys.csv', skiprows=0, low_memory=False)\n",
    "positive_emoticons = emoticons[emoticons.Sentiment == 1]\n",
    "negative_emoticons = emoticons[emoticons.Sentiment == 0]\n",
    "# Load Acronyms Dataset\n",
    "acronyms = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Acronyms.csv', skiprows=0, low_memory=False)\n",
    "# Load Stopwords Dataset\n",
    "stops = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Stopwords.csv', skiprows=0, low_memory=False)\n",
    "stops.columns = ['Word']\n",
    "# Load Positive Words Dataset\n",
    "positive_words = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Positive-words.csv', skiprows=0, low_memory=False ,sep='\\t')\n",
    "positive_words.columns = ['Word', 'Sentiment']\n",
    "# Load Negative Words Dataset\n",
    "negative_words = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Negative-words.csv', skiprows=0, low_memory=False ,sep='\\t',encoding= 'unicode_escape')\n",
    "negative_words.columns = ['Word', 'Sentiment']\n",
    "# Load Negations Dataset\n",
    "negation_words = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BE_datasets\\Negation.csv', skiprows=0, low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the Data\n",
    "consumerkey= ''\n",
    "consumersecret=''\n",
    "accesstoken=''\n",
    "accesstokensecret=''\n",
    "\n",
    "\n",
    "# Create the authentication object\n",
    "authenticate= tweepy.OAuthHandler(consumerkey, consumersecret)\n",
    "# Set the access token and access token secret\n",
    "authenticate.set_access_token(accesstoken,accesstokensecret)\n",
    "# Create the API object while passing in the auth information\n",
    "api=tweepy.API(authenticate, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "twitterusername=input(\"Type the twitter username that you want to analyze his or her tweets: \")\n",
    "tweetnumber=input(\"How many tweets do you want to extract? \")\n",
    "\n",
    "\n",
    "\n",
    " # Extracting 100 Tweets from the twitter user\n",
    "posts= api.user_timeline(screen_name=twitterusername, count=tweetnumber, lang=\"en\", tweet_mode=\"extended\")\n",
    "\n",
    "\n",
    "# Create a dataframe with a column called Tweets\n",
    "Newtweets = pd.DataFrame( [tweet.full_text for tweet in posts] , columns=['Tweets'])\n",
    "\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "Newtweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# We should do all preprocessing that was already done to the dataset we already applied our classifier to\n",
    "\n",
    "# Remove RTs\n",
    "def cleanTxt(text):\n",
    "    text= re.sub(r'RT[\\s]+', '', text)\n",
    "    text= re.sub(r'[0-9]+', '', text) \n",
    "    return text\n",
    "# Cleaning the text\n",
    "Newtweets['Tweets']= Newtweets['Tweets'].apply(cleanTxt)\n",
    "\n",
    "import re # Regular Expressions\n",
    "\n",
    "# Creating the functions to detect and replace emoticons  \n",
    "\n",
    "def make_emoticon_pattern3(emoticons):\n",
    "    pattern = \"|\".join(map(re.escape, emoticons.Smiley))\n",
    "    pattern = \"(?<=\\s)(\" + pattern + \")(?=\\s)\"\n",
    "    return pattern\n",
    "\n",
    "def find_with_pattern3(pattern, replace=False, tag=None):\n",
    "    if replace and tag == None:\n",
    "        raise Exception(\"Parameter error\", \"If replace=True you should add the tag by which the pattern will be replaced\")\n",
    "    regex = re.compile(pattern)\n",
    "    if replace:\n",
    "        return Newtweets.Tweets.apply(lambda tweet: re.sub(pattern, tag, \" \" + tweet + \" \"))\n",
    "    return Newtweets.Tweets.apply(lambda tweet: re.findall(pattern, \" \" + tweet + \" \"))\n",
    "\n",
    "pos_emoticons_found = find_with_pattern3(make_emoticon_pattern3(positive_emoticons))\n",
    "neg_emoticons_found = find_with_pattern3(make_emoticon_pattern3(negative_emoticons))\n",
    "\n",
    "nb_pos_emoticons = len(pos_emoticons_found[pos_emoticons_found.map(lambda emoticons : len(emoticons) > 0)])\n",
    "nb_neg_emoticons = len(neg_emoticons_found[neg_emoticons_found.map(lambda emoticons : len(emoticons) > 0)])\n",
    "\n",
    "\n",
    "Newtweets.Tweets = find_with_pattern3(make_emoticon_pattern3(positive_emoticons), True, '||pos||')\n",
    "Newtweets.Tweets = find_with_pattern3(make_emoticon_pattern3(negative_emoticons), True, '||neg||')\n",
    "\n",
    "# Replacing URLS\n",
    "\n",
    "pattern_url = re.compile(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "url_found = find_with_pattern3(pattern_url)\n",
    "\n",
    "Newtweets.Tweets = find_with_pattern3(pattern_url, True, '||url||')\n",
    "\n",
    "# Remove Unicode characters \n",
    "\n",
    "def remove_unicode3(text):\n",
    "    try:\n",
    "        text = text.encode('ascii','ignore')\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "Newtweets.Tweets = Newtweets.Tweets.apply(lambda tweet: remove_unicode3(tweet))\n",
    "\n",
    "def remove_unicode4(text):\n",
    "    try:\n",
    "        text = text.decode('unicode_escape')\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "Newtweets.Tweets = Newtweets.Tweets.apply(lambda tweet: remove_unicode4(tweet))\n",
    "\n",
    "# Simply decode HTML entities\n",
    "import html\n",
    "# Convert tweets in unicode utf-8 to avoid mixing unicode with ascii and causing an error during unescape\n",
    "Newtweets.Tweets  = Newtweets.Tweets.apply(lambda tweet: html.unescape(str(tweet)))\n",
    "\n",
    "# Reduce all letters to lower case\n",
    "Newtweets.Tweets = Newtweets.Tweets.str.lower()\n",
    "\n",
    "\n",
    "pattern_usernames = \"@\\w{1,}\"\n",
    "usernames_found3 = find_with_pattern3(pattern_usernames)\n",
    "\n",
    "# Replace all usernames/targets @ with the tag ||target||\n",
    "Newtweets.Tweets = find_with_pattern3(pattern_usernames, True, '||target||')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Create a dictionary of acronym which will be used to get translations\n",
    "acronym_dictionary = dict(zip(acronyms.Acronym, acronyms.Translation))\n",
    "\n",
    "# Will be used to get rid of the punctuation in tweets (does not include | since we use it for our tokens and ' \n",
    "# to take care of don't, can't)\n",
    "punctuation = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{}~'\n",
    "\n",
    "# Frequency table for acronyms\n",
    "acronyms_counter = Counter()\n",
    "\n",
    "# Loop on acronyms to replace those matched in the tweet by the corresponding translations\n",
    "# Return the tweet and the acronyms used\n",
    "def acronym_to_translation(tweet, acronyms_counter):\n",
    "    table = str.maketrans(punctuation,\" \" * len(punctuation))\n",
    "    tweet = str(tweet).translate(table)\n",
    "    words = tweet.split()\n",
    "    new_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        if acronym_dictionary.__contains__(word):\n",
    "            acronyms_counter[word] += 1\n",
    "            new_words.extend(acronym_dictionary[word].split())\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "Newtweets.Tweets = Newtweets.Tweets.apply(lambda tweet: acronym_to_translation(str(tweet), acronyms_counter))\n",
    "\n",
    "# Transform the dataframe into a dictionary\n",
    "negation_dictionary = dict(zip(negation_words.Negation, negation_words.Tag))\n",
    "\n",
    "# Find a negation in a tweet and replace it by its tag\n",
    "def replace_negation(tweet):\n",
    "    return [negation_dictionary[word] if negation_dictionary.__contains__(word) else word for word in tweet]\n",
    "    \n",
    "# Apply the function on every tweet\n",
    "Newtweets.Tweets = Newtweets.Tweets.apply(lambda tweet: replace_negation(tweet))\n",
    "\n",
    "#Replace a sequence of repeated characters by two characters\n",
    "pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "def reduce_sequence_word(word):\n",
    "    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n",
    "\n",
    "def reduce_sequence_tweet(tweet):\n",
    "    return [reduce_sequence_word(word) for word in tweet]\n",
    "\n",
    "Newtweets.Tweets = Newtweets.Tweets.apply(lambda tweet: reduce_sequence_tweet(tweet))\n",
    "\n",
    "# Transform the dataframe into a dictionary\n",
    "stopword_dictionary = dict.fromkeys(stops.Word, None)\n",
    "\n",
    "# Rejoin the tweets to start testing\n",
    "Newtweets.Tweets = Newtweets.Tweets.apply(lambda tweet: \" \".join(tweet))\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "Newtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load the pkl file that has the best model with its countvectorizer function to test it on our new data\n",
    "# This was recorded as the best model that reached approximatly F1 score of 80% of all folds, and having recorded the one with the best fold in here\n",
    "pkl_filename=(r'C:\\Users\\USER\\Desktop\\BE\\Models\\Multi_unibi_before_ss2.pkl')\n",
    "# Load the model and the count vectorizer to start using them\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    pickle_cv=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The Word Cloud\n",
    "allWords = ' '.join( [twts for twts in Newtweets['Tweets']])\n",
    "wordCloud = WordCloud(width = 500, height=300, random_state = 21, max_font_size= 119).generate(allWords)\n",
    "plt.imshow(wordCloud, interpolation= \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keyword=input(\"Type a word that you want your tweets to be related to: \")\n",
    "FilteredTweets=Newtweets[Newtweets['Tweets'].str.contains(Keyword)]\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "FilteredTweets = FilteredTweets.reset_index()\n",
    "FilteredTweets = FilteredTweets.drop('index', 1)\n",
    "FilteredTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection of the Filtered tweets\n",
    "\n",
    "# Transform our new data into bag of words using the countvectoroizer function applied in the loaded model\n",
    "transformedtweets_filtered=pickle_cv.transform(FilteredTweets['Tweets'])\n",
    "testpredictions_filtered=pickle_model.predict(transformedtweets_filtered)\n",
    "# Transform our sentiments detected into a dataframe \n",
    "testpred_dt_filtered=pd.DataFrame(testpredictions_filtered, columns=['Sentiment']) \n",
    "# Join the tweets and their corresponding detected sentiment\n",
    "FilteredTweets['Sentiment']= testpred_dt_filtered['Sentiment']\n",
    "FilteredTweets.Sentiment = FilteredTweets.Sentiment.astype(float)\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "FilteredTweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection of all tweets retrieved\n",
    "\n",
    "# Transform our new data into bag of words using the countvectoroizer function applied in the loaded model\n",
    "transformedtweets=pickle_cv.transform(Newtweets['Tweets'])\n",
    "testpredictions=pickle_model.predict(transformedtweets)\n",
    "# Transform our sentiments detected into a dataframe \n",
    "testpred_dt=pd.DataFrame(testpredictions, columns=['Sentiment']) \n",
    "# Join the tweets and their corresponding detected sentiment\n",
    "Newtweets['Sentiment']= testpred_dt['Sentiment']\n",
    "Newtweets.Sentiment = Newtweets.Sentiment.astype(float)\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "Newtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
